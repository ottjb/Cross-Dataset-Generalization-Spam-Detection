{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f204fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f67fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class EmailSpamDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = f\"Subject: {row['subject']} [SEP] Body: {row['body']}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'url_feature': torch.tensor(row['urls'], dtype=torch.float32),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67036532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "\n",
    "class SpamDetectionTransformer(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2, dropout=0.3):\n",
    "        super(SpamDetectionTransformer, self).__init__()\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "\n",
    "        self.feature_fc = nn.Linear(1, 32)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, url_feature):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        url_embedding = torch.relu(self.feature_fc(url_feature.unsqueeze(1)))\n",
    "        combined = torch.cat([pooled_output, url_embedding], dim=1)\n",
    "\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1eddb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        url_feature = batch['url_feature'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask, url_feature)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "563b006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Evaluating')\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            url_feature = batch['url_feature'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, url_feature)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='binary'\n",
    "    )\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99b460cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data\n",
      "Filling 33297 empty senders\n",
      "Removing receiver (not useful for classification)\n",
      "Dropping 0 rows with both subject and body missing\n",
      "Filling 503 empty subjects\n",
      "Filling 1 empty bodies\n",
      "Filling 32626 empty URLs\n",
      "Model loaded with 66601154 parameters\n"
     ]
    }
   ],
   "source": [
    "# Preperation of data and model\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "CEAS_08 = pd.read_csv(\"datasets/CEAS_08.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "ENRON = pd.read_csv(\"datasets/Enron.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "LING = pd.read_csv(\"datasets/Ling.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NAZARIO = pd.read_csv(\"datasets/Nazario.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NAZARIO_5 = pd.read_csv(\"datasets/Nazario_5.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NIGERIAN_FRAUD = pd.read_csv(\"datasets/Nigerian_Fraud.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NIGERIAN_5 = pd.read_csv(\"datasets/Nigerian_5.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "SPAMASSASSIN = pd.read_csv(\"datasets/SpamAssasin.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "dfs = [CEAS_08, ENRON, LING, NAZARIO, NAZARIO_5, NIGERIAN_FRAUD, NIGERIAN_5, SPAMASSASSIN]\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Cleaning Data\")\n",
    "\n",
    "print(f\"Filling {df['sender'].isnull().sum()} empty senders\")\n",
    "df['sender'] = df['sender'].fillna(\"[NO_SENDER]\")\n",
    "\n",
    "print(\"Removing receiver (not useful for classification)\")\n",
    "df = df.drop('sender', axis=1)\n",
    "df = df.drop('receiver', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "\"\"\" df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True)\n",
    "date_missing = df['date'].isnull().sum()\n",
    "print(f\"Filling {date_missing} dates\")\n",
    "if date_missing > 0:\n",
    "    median_date = df['date'].median()\n",
    "    df['date'] = df['date'].fillna(median_date) \"\"\"\n",
    "\n",
    "both_missing = df['subject'].isnull() & df['body'].isnull()\n",
    "print(f\"Dropping {both_missing.sum()} rows with both subject and body missing\")\n",
    "if both_missing.sum() > 0:\n",
    "    print(\"Dropping rows with no text content at all...\")\n",
    "    df = df[~both_missing]\n",
    "\n",
    "print(f\"Filling {df['subject'].isnull().sum()} empty subjects\")\n",
    "df['subject'] = df['subject'].fillna('[NO_SUBJECT]')\n",
    "\n",
    "print(f\"Filling {df['body'].isnull().sum()} empty bodies\")\n",
    "df['body'] = df['body'].fillna('[NO_BODY]')\n",
    "\n",
    "print(f\"Filling {df['urls'].isnull().sum()} empty URLs\")\n",
    "df['urls'] = df['urls'].apply(\n",
    "    lambda x: 0 if x == '[]' or x == 0 else 1\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "train_dataset = EmailSpamDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmailSpamDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmailSpamDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = SpamDetectionTransformer(model_name=MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b02836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preperation for training\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a441859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/3\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 100/4020 [14:10<9:32:26,  8.76s/it, loss=0.684]"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "best_val_f1 = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Val Precision: {val_prec:.4f} | Val Recall: {val_rec:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_spam_model.pt')\n",
    "        print(f\"✓ Saved best model with F1: {val_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
