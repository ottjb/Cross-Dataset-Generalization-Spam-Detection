{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f204fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ottja\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Device: cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f67fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "\n",
    "class EmailSpamDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = f\"Subject: {row['subject']} [SEP] Body: {row['body']}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'url_feature': torch.tensor(row['urls'], dtype=torch.float32),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67036532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "\n",
    "class SpamDetectionTransformer(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', num_classes=2, dropout=0.3):\n",
    "        super(SpamDetectionTransformer, self).__init__()\n",
    "\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "\n",
    "        self.feature_fc = nn.Linear(1, 32)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size + 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, url_feature):\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        url_embedding = torch.relu(self.feature_fc(url_feature.unsqueeze(1)))\n",
    "        combined = torch.cat([pooled_output, url_embedding], dim=1)\n",
    "\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1eddb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        url_feature = batch['url_feature'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask, url_feature)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563b006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Evaluating')\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            url_feature = batch['url_feature'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, url_feature)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='binary'\n",
    "    )\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b460cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data\n",
      "Filling 33297 empty senders\n",
      "Removing receiver (not useful for classification)\n",
      "Dropping 0 rows with both subject and body missing\n",
      "Filling 503 empty subjects\n",
      "Filling 1 empty bodies\n",
      "Filling 32626 empty URLs\n",
      "Model loaded with 66601154 parameters\n"
     ]
    }
   ],
   "source": [
    "# Preperation of data and model\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "CEAS_08 = pd.read_csv(\"datasets/CEAS_08.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "ENRON = pd.read_csv(\"datasets/Enron.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "LING = pd.read_csv(\"datasets/Ling.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NAZARIO = pd.read_csv(\"datasets/Nazario.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NAZARIO_5 = pd.read_csv(\"datasets/Nazario_5.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NIGERIAN_FRAUD = pd.read_csv(\"datasets/Nigerian_Fraud.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "NIGERIAN_5 = pd.read_csv(\"datasets/Nigerian_5.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "SPAMASSASSIN = pd.read_csv(\"datasets/SpamAssasin.csv\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "dfs = [CEAS_08, ENRON, LING, NAZARIO, NAZARIO_5, NIGERIAN_FRAUD, NIGERIAN_5, SPAMASSASSIN]\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Cleaning Data\")\n",
    "\n",
    "print(f\"Filling {df['sender'].isnull().sum()} empty senders\")\n",
    "df['sender'] = df['sender'].fillna(\"[NO_SENDER]\")\n",
    "\n",
    "print(\"Removing receiver (not useful for classification)\")\n",
    "df = df.drop('sender', axis=1)\n",
    "df = df.drop('receiver', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "\"\"\" df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True)\n",
    "date_missing = df['date'].isnull().sum()\n",
    "print(f\"Filling {date_missing} dates\")\n",
    "if date_missing > 0:\n",
    "    median_date = df['date'].median()\n",
    "    df['date'] = df['date'].fillna(median_date) \"\"\"\n",
    "\n",
    "both_missing = df['subject'].isnull() & df['body'].isnull()\n",
    "print(f\"Dropping {both_missing.sum()} rows with both subject and body missing\")\n",
    "if both_missing.sum() > 0:\n",
    "    print(\"Dropping rows with no text content at all...\")\n",
    "    df = df[~both_missing]\n",
    "\n",
    "print(f\"Filling {df['subject'].isnull().sum()} empty subjects\")\n",
    "df['subject'] = df['subject'].fillna('[NO_SUBJECT]')\n",
    "\n",
    "print(f\"Filling {df['body'].isnull().sum()} empty bodies\")\n",
    "df['body'] = df['body'].fillna('[NO_BODY]')\n",
    "\n",
    "print(f\"Filling {df['urls'].isnull().sum()} empty URLs\")\n",
    "df['urls'] = df['urls'].apply(\n",
    "    lambda x: 0 if x == '[]' or x == 0 else 1\n",
    ")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "train_dataset = EmailSpamDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = EmailSpamDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = EmailSpamDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = SpamDetectionTransformer(model_name=MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02836c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preperation for training\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a441859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "best_val_f1 = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Val Precision: {val_prec:.4f} | Val Recall: {val_rec:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'models/best_spam_model.pt')\n",
    "        print(f\"✓ Saved best model with F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067dab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 647/647 [17:32<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Test Set Results\n",
      "==================================================\n",
      "Test Loss: 0.0100\n",
      "Test Accuracy: 0.9976\n",
      "Test Precision: 0.9976\n",
      "Test Recall: 0.9978\n",
      "Test F1 Score: 0.9977\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4953   13]\n",
      " [  12 5361]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       1.00      1.00      1.00      4966\n",
      "        Spam       1.00      1.00      1.00      5373\n",
      "\n",
      "    accuracy                           1.00     10339\n",
      "   macro avg       1.00      1.00      1.00     10339\n",
      "weighted avg       1.00      1.00      1.00     10339\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    model.load_state_dict(torch.load('models/best_spam_model.pt', map_location=torch.device('cpu')))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/best_spam_model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1, predictions, true_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Set Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_prec:.4f}\")\n",
    "print(f\"Test Recall: {test_rec:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report\")\n",
    "print(classification_report(true_labels, predictions, target_names=['Ham', 'Spam']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
